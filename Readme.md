
# ğŸ§  FinWizard Data Pipeline

A modular and extensible data engineering pipeline to fetch, process, and store historical market data for equities and indices â€” designed with high-frequency, low-latency financial applications in mind.

---

## ğŸš€ Overview

This project implements a **production-grade ETL pipeline** in Python to collect historical minute-level data from Zerodha's Kite API, enrich it with technical indicators, and store it in Parquet format for high-performance analytics and backtesting.

It mimics the architecture often seen in quantitative trading and research systems, focusing on **scalability, modularity, and precision**.

---

## ğŸ“ Project Structure

```
FINWIZARDDATAPIPELINE/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ constants.py         # API keys, date config, storage paths
â”‚
â”œâ”€â”€ data/                    # Output directory for processed data
â”‚   â”œâ”€â”€ BAJAJFINSV/
â”‚   â””â”€â”€ dummy/
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_handler.py      # ETL orchestrator
â”‚   â”œâ”€â”€ indicators.py        # Technical indicators (RSI, MACD, etc.)
â”‚   â””â”€â”€ kite_api.py          # Historical data wrapper for Kite API
â”‚
â”œâ”€â”€ main.py                  # Entry point (optional)
â”œâ”€â”€ requirements.txt         # Project dependencies
â””â”€â”€ .gitignore
```

---

## ğŸ§© Core Features

- âœ… **Historical data ingestion** using Zerodha Kite HTTP API
- ğŸ“ˆ **On-the-fly technical computation**: RSI, MACD, ATR, Bollinger Bands, VWAP, EMAs
- ğŸ’¾ **Parquet-based storage** for fast columnar queries
- ğŸ” **Incremental updates**: avoids redundant downloads
- ğŸ” Designed with **thread-safe file operations**, but currently runs sequentially

---

## âš™ï¸ How It Works

1. **Pull minute-level OHLCV+OI data** from the Kite API
2. **Calculate indicators** on-the-fly using rolling/statistical methods
3. **Write to Parquet files**, chunked by date range

---

## ğŸ“Š Output Format

Sample columns:

| date       | time     | open  | high  | low   | close | RSI   | MACD  | VWAP  |
|------------|----------|-------|-------|-------|-------|--------|--------|--------|
| 2024-06-01 | 09:15:00 | 715.0 | 716.5 | 714.2 | 716.0 | 52.40 | 1.23  | 715.7  |

Data is clean, enriched, and optimized for downstream consumption by:

- Backtesters
- ML pipelines
- Trading signal engines
- Exploratory dashboards

---

## âœ… Usage

### 1. Install requirements

```bash
pip install -r requirements.txt
```

### 2. Configure your settings in `config/constants.py`

```python
BASE_PATH = './data/'
USER_ID = 'XXXXXX'
HEADERS = {
    "Authorization": "enctoken XXXXXXXXX",
    "X-Kite-Version": "3"
}
START_DATE_DEFAULT = datetime(2024, 6, 1)
END_DATE_MASTER = datetime(2024, 6, 25)
```

### 3. Run the pipeline

```python
from utils.data_handler import update_index_data

update_index_data(index_name="BAJAJFINSV", token=123456)
```

---

## ğŸ“Œ Why This Project?

- ğŸ§  Demonstrates fluency in **financial data structures** and **timeseries processing**
- ğŸ“¦ Focuses on **efficient storage** and scalable design
- ğŸ” Highlights capability in **building real-world data infrastructure**
- ğŸ§© Built to plug directly into backtesters, dashboards, or strategy layers

---

## ğŸ› ï¸ Possible Extensions

- [ ] Add CLI or scheduler for automation
- [ ] Extend to live streaming mode with WebSockets
- [ ] Add retry logic, failover, and metadata tracking
- [ ] Build a web dashboard for visual insights
- [ ] Introduce unit tests and coverage reports

---

## ğŸ§¾ License

MIT License. Built for educational and analytical purposes.

---

## ğŸ™‹ Reach Out

Happy to explain design decisions, use cases, and potential improvements.
